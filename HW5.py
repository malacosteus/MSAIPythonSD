# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
https://colab.research.google.com/drive/1uaB4WEbsTBOOfQiEVGNMlgy6YfYzAYZo

https://en.wikipedia.org/wiki/Software_design_pattern

"""

"""
**Task 1:**
The task is to use one of creational patterns from the article above in a practical data science task.

You are to analyze how train dataset size affects the score of your model. Thus, you're supposed to create subsamples based on the initial dataset.

As part of this task, you may train classical linear regression on the iris dataset. (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)

It's necessary to print a range of subsample sizes and corresponding model scores.
"""

from sklearn.datasets import load_iris

from sklearn.utils import shuffle

from sklearn import tree

from sklearn import neighbors
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score

from sklearn.preprocessing import StandardScaler

import numpy as np

# classic implementation of Singleton Design pattern
class Singleton:
 
    __shared_instance = 'i1'
 
    @staticmethod
    def getInstance():
        '''Static Access Method'''
        if Singleton.__shared_instance == 'i1':
            Singleton()
        return Singleton.__shared_instance
 
    def __init__(self, X_train, y_train):
        '''virtual private constructor'''
        if Singleton.__shared_instance != 'i1':
            raise Exception ('This class is a singleton class !')
        else:
            Singleton.__shared_instance = self
            self.X_train = X_train
            self.y_train = y_train

    def get_subsample(self, df_share):
        '''
        1. Copy train dataset
        2. Shuffle data (don't miss the connection between X_train and y_train)
        3. Return df_share %-subsample of X_train and y_train
        '''
        ret_size = int((df_share / 100) * self.y_train.size)
        X, y = shuffle(self.X_train, self.y_train)
        return (X[:ret_size], y[:ret_size])

"""
1. Load iris dataset
2. Shuffle data and divide into train / test.
"""
iris = load_iris()
X_train, X_test, y_train, y_test=train_test_split(iris.data, iris.target, test_size=0.2, shuffle=True)

pattern_item = Singleton(X_train, y_train)
for df_share in range(10, 101, 10):
    """
    1. Preprocess curr_X_train, curr_y_train in the way you want
    2. Train Linear Regression on the subsample
    3. Save or print the score to check how df_share affects the quality
    """
    curr_X_train, curr_y_train = pattern_item.get_subsample(df_share)

    scaler = StandardScaler()
    classifier = LogisticRegression()
    scaler.fit(curr_X_train)
    curr_X_train = scaler.transform(curr_X_train)

    classifier.fit(curr_X_train, curr_y_train)
    predictions = classifier.predict(scaler.transform(X_test))
    
    print(f'Number of samples = {curr_X_train.size}; Accuracy score  = {accuracy_score(y_test, predictions):.2f}')

"""What pattern did you use? Explain your choice of pattern

**Answer**

We choose to use Singleton pattern for this task.
Singletone ensures that a class has only one instance, and provide a global point of access to it. We may want to use it because we test our ML algorithm with different parameters. Here we tested it sequentially but in real world scenarious testing may be parallelized. In this case we would want to insure that our data stays the same because if in one of paralell tests will change our data our results will be wrong. Singleton pattern can prevent this by trowing Exception for example.
"""


"""
**Task2:**
The task is to use one of structural patterns from the article above in a practical data science task.

In this section you're supposed to implement an ensemble for classification problem Â– a wrapper over a set of classifiers.

As part of this task, you may train an ensemble on the iris dataset. (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)
"""

class FacadePattern:
    def __init__(self, models = [LogisticRegression(), neighbors.KNeighborsClassifier(), tree.DecisionTreeClassifier()]) -> None:
        '''
        Initialize a class item with a list of classificators
        '''

        self.X_train = X_train
        self.y_train = y_train
        self.models = models 

        self.scaler = StandardScaler()


    def fit(self, X_train, y_train) -> None:
        '''
        Fit classifiers from the initialization stage
        '''
        self.scaler.fit(X_train)
        X_train = self.scaler.transform(X_train)

        for m in self.models:
            m.fit(X_train, y_train)

    def predict(self, X):
        '''
        Get predicts from all the classifiers and return
        the most popular answers
        '''
        X = self.scaler.transform(X)
        s1, _ = X.shape
        res = np.zeros(s1)
        for m in self.models:
            res = res + m.predict(X)

        res = res // len(self.models)
        return res
"""
1. Load iris dataset
2. Shuffle data and divide into train / test.
3. Prepare classifiers to initialize <StructuralPatternName> class.
4. Train the ensemble
"""

iris = load_iris()
X_train, X_test, y_train, y_test=train_test_split(iris.data, iris.target, test_size=0.2, shuffle=True)

classifier = FacadePattern()

classifier.fit(X_train, y_train)
predictions = classifier.predict(X_test)
    
print(f'Accuracy scre  = {accuracy_score(y_test, predictions):.2f}')

"""What pattern did you use? Explain your choice of pattern

**Answer**

We used Facacde pattern for this task.

Facade Method is a pattern that provides a simpler unified interface to a more complex system. The word Facade means the face of a building or particularly an outer lying interface of a complex system, consists of several sub-systems. It provides an easier way to access methods of the underlying systems by providing a single entry point.

In our case we use multiple algorithms and later combine output of all algorithms to make one prediction. So our final algorithm consist of multiple different sub-algorithms which we need to combine in one facade. Because of that Facde pattern is good fit for our task.

**Task3:**
The task is to use one of the behavioral patterns from the article above in a practical data science task.

You're supposed to train a neural network with pretrained weights. You have a series of experiments, but if one of them fails, you need to restore the previous state of the network. That way you'll apply only successful experiments and achieve better score. Use one of the behavioral patterns to provide functionality for it.

In this task you have a very simplified neural network. But still it can be trained with your help. Add some code to make loss go down.
"""

import random
from numpy.random import randint

# We will use the Memento pattern

# memento class
class Memento:
    def __init__(self, state) -> None:
        self._state = state

    def get_saved_state(self):
        return self._state


def get_random_vector(n):
    '''Return `n` floats from -1 to 1.'''
    return [random.random() * 2 - 1 for _ in range(n)]


# Originator class, serving as caretaker-class as well
class DumbNeuralNetwork():
    def __init__(self, weights_count):
        self.weights_count = weights_count
        self.weights = get_random_vector(self.weights_count)

    def train(self):
        random_vector = get_random_vector(self.weights_count)
        for i in range(self.weights_count):
            self.weights[i] += random_vector[i]

    def predict(self, X_test):
        return [
            sum(feature * coef for feature, coef in zip(x, self.weights))
            for x in X_test
        ]

    def evaluate(self, X_test, y_test):
        y_predicted = self.predict(X_test)
        loss_sum = sum(abs(y1 - y2) for y1, y2 in zip(y_predicted, y_test))
        loss_average = loss_sum / self.weights_count
        return loss_average

    def save_weights(self):
        '''
        Save weights to restore them later.
        '''
        save_wghts = Memento(self.weights.copy())
        return save_wghts


    def restore_weights(self, mmnt):
        '''
        Restore weights saved previously.
        '''
        self.weights = mmnt.get_saved_state()


weights_count = 1000
dumb_NN = DumbNeuralNetwork(weights_count)

test_samples = 100
# [[<float -1..1>, ...], ...]
X_test = [get_random_vector(weights_count) for _ in range(test_samples)]
# [<float -2000..2000>, ...]
y_test = [value * weights_count * 2 for value in get_random_vector(test_samples)]

epoch_number = 10000
best_loss = weights_count * 2

for epoch in range(epoch_number):
    memento = dumb_NN.save_weights()
    dumb_NN.train()

    loss = dumb_NN.evaluate(X_test, y_test)
    print(f"epoch = {epoch}; loss = {loss}; best loss = {best_loss}")
    
    if loss < best_loss:
        best_loss = loss
    else:
        print(f'rolling back...')
        dumb_NN.restore_weights(memento)
    
print(f'Result: {best_loss}=')

"""What pattern did you use? Explain your choice of pattern

**Answer**

The memento pattern is a software design pattern that provides the ability to restore an object to its previous state (undo via rollback).

It's exactly that we need in this case. We save weights of our net into memento object of Memento class. And later we can restore our  Network state from this memnto object
"""